\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{amsmath}

\linespread{1.2}
\usepackage{parskip}
\setlength{\parindent}{15pt}

\newcommand{\real}{\mathbb{R}}
\newcommand{\sysIIN}{\mbox{$I \overset{N}{\rightarrow} I$}}
\newcommand{\sysII}{\mbox{$I \rightarrow I$}}
\newcommand{\sysIJN}{\mbox{$I \overset{N}{\rightarrow} J$}}
\newcommand{\sysIJ}{\mbox{$I \rightarrow J$}}
\newcommand{\sysJJN}{\mbox{$J \overset{N}{\rightarrow} J$}}
\newcommand{\sysJJ}{\mbox{$J \rightarrow J$}}

\begin{document}

\section{Problem Description and Background}
% Give an overview of the section: We will first give a more well defined problem description will well defined terms and present the problem as an optimization problem. We will mention the parallels between out problem and regular VQ and COVQ. We will adapt the methods to our problem. Mention how we can use conditions of optimality in quantizer design, also mention the bit allocation problem and how the LBG splitting algorithm can resolve the issues. We will discuss (in following section) conditions of optimality in an optimal quantizer for vq, covq, and the joint decoder system. We will introduce optimal codeword assignment for a noisy channel and then mention Bit allocation and transform coding techniques used in image coding.
\begin{enumerate}
    \item Looking at the transmission of images, across possibly noisy channels, and this involves the use of encoders and decoders to compress the amount of information.
    \item 

\end{enumerate}
\sysII, \sysIIN, \sysIJ, \sysIJN, \sysJJ, \sysJJN, test.


This section provides background related to the independent encoder/independent decoder, independent encoders/joint decoder, and joint encoder/joint decoder schemes mentioned in the previous section. As previously mentioned, analysis of each system is important so they can be quantitatively compared. Recall that the two independent encoder/two dependent decoder system provides a lower bound on performance for the joint decoder system, while the single encoder/single decoder system provides an upper bound. The formal problem statement for the different systems and a glossary of terms is covered in Section~\ref{sec:prob_state}.

In addition to covering these three different systems, we will also be analyzing noiseless and noisy channels. There are a few additional design consideration one must consider when dealing with a noisy channel. In particular, it is important that a proper mapping between codeword indices and respective binary representations is chosen. This is because transition probabilities for the indices depend only on the binary representation of the indices. Equations for conditions of optimality of the different systems are given in Section~\ref{sec:optim_conds_deriv}. Analysis of codeword assignment for noisy channels is done is Section~\ref{sec:code_assign_deriv}.

Finally, quantizer bit allocation and transform coding will be covered in Section~\ref{sec:bit_trans_deriv}. The material will be covered with an emphasis on image encoding, since this is the focus of the project. Applications to the JPEG image compression method will also be mentioned.

\subsection{Problem Statement}
\label{sec:prob_state}
% This section should reintroduce the problem while providing the list of terms. In particular, this should include codevector, index, channel transition probability, distortion.
Assume that the source random vector $(X,Y)\in\real^2$ is independent and identically distributed, such that $X$ and $Y$ share a joint density function. The quantizer maps the values of $X$ and $Y$ onto a finite set of indices: $i\in\{1,\ldots,N_X\}$ for $X$ and $j\in\{1,\ldots,N_Y\}$ for $Y$ where $N_X$ and $N_Y$ are the number of \emph{output levels} for $X$ and $Y$ respectively.

For the independent encoder systems \sysII, \sysIIN, \sysIJ, and \sysIJN, the encoders for the two sources map $X$ and $Y$ onto indices $i$ and $j$ independently. These encoder mappings are denoted
\begin{gather*}
    E_X : \real\to\{1,\ldots,N_X\} \\
    E_Y : \real\to\{1,\ldots,N_Y\}
\end{gather*}
where $E_X$ is the encoder mapping for $X$, and $E_Y$ is the encoder mapping for Y.

For the joint encoder systems \sysJJ\ and \sysJJN, the encoders jointly maps $X$ and $Y$ onto a pair of indices and can be described as a mapping
\begin{equation*}
    E : \real^2\to\{1,\ldots,N_X\} \times \{1,\ldots,N_Y\}
\end{equation*}
Notice that a joint encoder mapping $E$ can be used to describe any pair of independent encoder mappings $E_X$ and $E_Y$ if $E(X,Y) = (E_X(X),E_Y(Y))$. The converse, however, is not true.

After $X$ and $Y$ are encoded onto indices $i$ and $j$, they are transmitted over the channel to the receiver. If the channel is noiseless, the transmitted indices $i$ and $j$ will be received. For a noisy channel, channel interference will influence the received indices. In this project, we will consider channels that are both independent of the source and memoryless. The two channels themselves, however, may be dependent. Such channels can be expressed using channel transition probabilities $P(k,l|i,j)$. This corresponds to the probability of receiving index $k$ given index $i$ was transmitted from the $X$ source, and receiving index $l$ given index $j$ was transmitted from the $Y$ source. Note that the channel transition probabilities for the noiseless channel are given by $P(i,j|i,j)=1$ for all $i$ and $j$. Throughout the rest of the paper, we will use the convention that $i$ and $j$ correspond to the transmitted indices and $k$ and $l$ correspond to the received indices for $X$ and $Y$ respectively.

When the indices are received at the receiver, they are then decoded to construct estimates of $X$ and $Y$. For independent decoders, the estimate of $X$ only depends on the received index from the $X$ source $k$, and the estimate for $Y$ only depends on the received index from the $Y$ source $l$. The sets of estimates, or \emph{codebook}, for $X$ and $Y$ are given by
\begin{gather*}
    \{x_k \in \real : k = 1,\ldots,N_X\} \\
    \{y_l \in \real : l = 1,\ldots,N_Y\}
\end{gather*}
where $x_k$ is the estimate of $X$ at the decoder given index $k$ was received from the $X$ source, and $y_l$ is the estimate of $Y$ given index $l$ was received from the $Y$ source. We call the elements of the codebook \emph{codevectors}.

When using joint decoders, the estimates for $X$ and $Y$ will depend on both $k$ and $l$. The codebook for the joint decoder for $X$ and $Y$ can be described as the sets
\begin{gather*}
    \{x_{k,l} \in \real : k = 1,\ldots,N_X, l = 1,\ldots,N_Y\} \\
    \{y_{k,l} \in \real : k = 1,\ldots,N_X, l = 1,\ldots,N_Y\} \\
\end{gather*}
where the codevectors $x_{k,l}$ and $y_{k,l}$ are the decoder's estimate of $X$ and $Y$ given index $k$ was received from the $X$ source and index $l$ was received from the $Y$ source.

Notice that any independent codebook $\{x_k : k = 1,\ldots,N_X\}$ and $\{y_l : l = 1,\ldots,N_Y\}$ can be obtained with the joint codebook $\{x_{k,l'}=x_k : k=1,\ldots,N_X, l'=1,\ldots,N_Y\}$ and $\{y_{k',l}=y_l : k'=1,\ldots,N_X, l=1,\ldots,N_Y\}$.

In quantizer design, the goal is to minimize the average distortion between the source and estimate at the receiver with respect to some defined distortion measure. In this project, we will consider squared error distortion, so the average distortion between $X$ and $Y$ can be expressed as
\begin{equation}
    \label{eq:D_avg}
    D_{avg} = E[{(X-\hat{X})}^2 + {(Y-\hat{Y})}^2]
\end{equation}
where $\hat{X}$ and $\hat{Y}$ represent the estimates of $X$ and $Y$ at the receiver. An \emph{optimal quantizer} is one which minimizes Equation~\ref{eq:D_avg} across all other quantizers of the same number of output levels in $X$ and $Y$.

Explicit expressions for $D_{avg}$ are given in subsection 1.2 for the following following six systems: \sysII, \sysIIN, \sysIJ, \sysIJN, \sysJJ, \sysJJN\@.

\subsection{Explicit Expressions for Average Distortion}
We can re express Equation~\ref{eq:D_avg} in more meaningful ways for our different systems. For the independent decoders and encoders with a noiseless channel,

{\sc \noindent Independent Encoders And Decoders, No Noise (\sysII):}
\begin{equation}
    \label{eq:dist_indep_nonoise}
    D_{avg} = \sum_{i=1}^{N_X}E[{(X-x_i)}^2 | E_X(X) = i]P(E_X(X) = i) + \sum_{j=1}^{N_Y}E[{(Y-y_j)}^2 | E_Y(Y) = j]P(E_Y(Y) = j)
\end{equation}
say something.

{\sc \noindent Independent Encoders And Decoders, With Noise (\sysIIN):}
\begin{equation}
    \label{eq:dist_indep_noise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y}
    E[{(X-x_{k})}^2 + {(Y-y_{l})}^2 | E_X(X)=i, E_Y(Y)=j]P(k,l|i,j)P(E_X(X)=i, E_Y(Y)=j)
\end{equation}

{\sc \noindent Independent Encoders And Decoders, With Noise (\sysIIN):}
\begin{equation}
    \label{eq:dist_joint_nonoise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | E_X(X)=i, E_Y(Y)=j]P(E_X(X) = i, E_Y(Y) = j)
\end{equation}
For a joint decoder and independent encoders with a noisy channel,
\begin{equation}
    \label{eq:dist_joint_noise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | E_X(X) = i, E_Y(Y) = j]P(k,l|i,j)P(E_X(X) = i, E_Y(Y) = j)
\end{equation}
For a joint decoder and joint encoder with a noiseless channel,
\begin{equation}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | E(X,Y) = (i,j)]P(E(X,Y) = (i,j))
\end{equation}
For a joint decoder and joint encoder with a noisy channel,
\begin{equation}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | E(X,Y) = (i,j)]P(k,l|i,j)P(E(X,Y) = (i,j))
\end{equation}
An important note is that the average distortion in Equation~\ref{eq:dist_indep_nonoise} can be minimized by optimizing the quantizers for the two sources separately. This is not true in Equation~\ref{eq:dist_indep_noise} however, unless we assume that the channels are independent so that $P(k,l|i,j) = P(k|i)P(l|j)$. We will therefore make this assumption about the channel to simplify the following analysis and implementation of the independent decoder system.

The problem that will be studied will be the design and implementation of the three systems
In the following subsection we will present some conditions of optimality that an optimum quantizer.

\subsection{Conditions of Optimality}
\label{sec:optim_conds_deriv}
% Begin with conditions of optimality for VQ, that is the centroid and nearest neighbour condition. Presenting them as theorems, that is, if the codevectors are fixed, then applying the nearest neighbour conditions provide and optimal quantizer and so on. Do the same for COVQ, and for the joint decoder. Recall for the joint decoder that the nearest neighbour theorem relies on the other encoder being fixed. Make sure to derive the conditions of optimality for the case when the channel is noiseless first.

\subsection{Codeword Assignment}
\label{sec:code_assign_deriv}
% Show how the distortion measure can be separated into two separate terms, with only one term that depends on the distortion. That way, we only need to minimize that term to minimize the overall distortion. Mention the complexity of the problem, and an approximate solution will be discussed later.

\subsection{Bit Allocation and Transform Coding}
\label{sec:bit_trans_deriv}
% Discuss bit allocation and transform coding in the context of images. Follow Julian's notes and the lecture slides. Mention how when we use scalar quantization, we want to allocate more bits to the components carrying more energy. We also want to transform the information before processing so we can get better performance.

\end{document}

