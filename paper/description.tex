\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{amsmath}

\linespread{1.2}
\usepackage{parskip}
\setlength{\parindent}{15pt}

\newtheorem{theorem}{Theorem}[section]

\newcommand{\real}{\mathbb{R}}
\newcommand{\sysIIN}{\mbox{$I \overset{N}{\rightarrow} I$}}
\newcommand{\sysII}{\mbox{$I \rightarrow I$}}
\newcommand{\sysIJN}{\mbox{$I \overset{N}{\rightarrow} J$}}
\newcommand{\sysIJ}{\mbox{$I \rightarrow J$}}
\newcommand{\sysJJN}{\mbox{$J \overset{N}{\rightarrow} J$}}
\newcommand{\sysJJ}{\mbox{$J \rightarrow J$}}

\begin{document}

\section{Introduction}

\section{Background}
This section provides the background material related to the project. In particular, scalar and vector quantization will be introduced in Section~\ref{sec:quantization}, along with conditions of optimally related to these schemes. In Section~\ref{sec:channel_optimized}, quantizers used for transmission of information over noisy channels will be discussed. In Section~\ref{sec:quant_design_algos}, it will be shown how the theory derived in the previous two subsections can be used in optimal quantizer design. In Section~\ref{sec:code_assign}, proper codeword assignment for transmission over noisy channels will be discussed. Finally, bit allocation and transform coding techniques with a focus on images will be covered in Section~\ref{sec:bit_alloc}.

After the above framework is introduced, a more formal problem description with theoretical results will be covered in the following section.

\subsection{Quantization}
\label{sec:quantization}
As previously mentioned, quantization is the process of mapping data from some large set onto a smaller, finite set. Quantization is a form of \emph{lossy compression}, which means that after the data is quantized, it can not be recovered exactly.

As the number of possible outputs, or \emph{output level}, of a quantizer is finite, each quantized point can be represented by an index over a finite range. A quantizer with $N$ distinct output levels is called an $N$-level quantizer. 

If the data originally lies in some space $\mathcal{S}$, the encoder $\mathcal{E}$ can be defined as a mapping
\begin{equation}
\mathcal{E} : \mathcal{S} \rightarrow \{1,\ldots,N\}
\end{equation}
The \emph{encoding region} associated with index $i$ is the set of points in $\mathcal{S}$ that the quantizer maps onto index $i$ and is denoted $R_i$. That is,
\begin{equation}
R_i = \{x \in \mathcal{S} : \mathcal{E}(x) = i\}
\end{equation}
Notice that the set of encoding regions partition $\mathcal{S}$. Moreover, the set of encoding region uniquely defines $\mathcal{E}$.

Each index output by the encoder has a corresponding point is $\mathcal{S}$ corresponding to the reconstruction of the quantized point. These points are called \emph{code vectors}, and the set of code vectors is called the \emph{codebook}, denoted by $\mathcal{C}$. The code vector corresponding to index $i$ is denoted by $x_i$ for all $i \in \{1,\ldots,N\}$.

Notice that any $N$-level quantizer can be defined by the encoder-codebook pair $(\mathcal{E}, \mathcal{C}$. Moreover, the quantizer mapping $Q : \mathcal{S} \rightarrow \mathcal{C}$ is given by
\begin{equation}
Q(x) = x_{\mathcal{E}(x)}
\end{equation}
Quantizer performance and optimal quantizers will not be discussed. A distortion measure $d(x,y) : \mathcal{S}^2 \rightarrow \mathbb{R}^+$ is used to measure the performance of a quantizer. The average distortion $D_{avg}$ for a random i.i.d. source $X$, distortion measure $d(x,y)$, and quantizer $Q$, is given by
\begin{equation}
  \label{eq_D_avg}
D_{avg} = E[d(X,Q(X))]
\end{equation}
An $N$-level quantizer is \emph{optimal} if its average distortion is less than or equal to the average distortion of all other $N$-level quantizer with respect to the given distortion measure and source distribution.

In general, solving for an optimal quantizer is very difficult. Moreover, the underlying source distribution may not be known when designing the quantizer, such as when quantizing images. To evaluate this problem, iterative algorithms can be used in quantizer design that can work on a training set of the source, rather than with the source distribution itself. Two useful conditions of optimality for quantizer design will now be discussed, and their application to quantizer design will be covered in Section~\ref{sec:quant_design_algos}.

Before discussing these conditions of optimality, and for the rest of the paper, it will be assumed that $\mathcal{S}=\mathbb{R}^n$. A quantizer with $n=1$ is called a \emph{scalar quantizer}, and a quantizer with $n \ge 1$ is called a \emph{vector quantizer}. The rate $R$ of an $N$-level vector quantizer, in bits, is
\begin{equation}
R = \frac{1}{n}\log_2 N
\end{equation}
It will be assumed that the distortion measure being used will be the \emph{squared-error distortion}, denoted ${\bf \| x - y \|}$ so that
\begin{align}
d(\bf x, \bf y) &= {\bf \| x - y \|} \\
&= \sum_{i=1}^n{(x_i - y_i)}^2
\end{align}
where $x_i$ and $y_i$ are the $i$th coordinates of $\bf x$ and $\bf y$. It should also be mentioned that the following conditions of optimality can be given for general distortion measures and input spaces, although this will not be discussed in this paper.

The first condition of optimality is the \emph{centroid condition}. The centroid condition states that each code vector should be placed at the centroid of its corresponding encoding region. The formal statement is as follows:
\begin{theorem}
Amongst the set of all $N$-level quantizers with encoding function $\mathcal{E}$, the quantizer with code vectors
\begin{align}
x_i &= E[X | X \in R_i] \\
&= \frac{ \int_{R_i}xf(x)dx }{ \int_{R_i}f(x)dx }
\end{align}
for all $i \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
It is important to note that although a quantizer satisfying the above condition is optimal, it is not necessarily uniquely optimal.

The second condition of optimality is the \emph{nearest neighbor condition}. The nearest neighbor condition states that the encoder mapping $\mathcal{E}$ should be chosen such that each point should be mapped onto the code vector which results in the lowest distortion. More formally, the nearest neighbor condition can be stated as follows:
\begin{theorem}
Consider all $N$-level quantizers with codebook $\mathcal{C}$. The quantizer with encoding regions satisfying
\begin{equation}
R_i \subset \{x : \| x - x_i \| \le \| x - x_j \| \text{ for all } j = 1,\ldots,N \}
\end{equation}
for all $i \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
Subset in the above theorem is indicate that ties between nearest neighbors can be broken arbitrary.

The centroid and nearest neighbor conditions provide necessary conditions of optimality for an optimal quantizer, although they are not sufficient. A quantizer that satisfies both conditions of optimality is called a Lloyd-Max quantizer. Another import remark to make is that the centroid condition specifies an optimal codebook for a fixed encoder, and the nearest neighbor condition specifies and optimal encoder for a fixed codebook. Using conditions of optimality in quantizer design will be discussed in Section~\ref{sec:quant_design_algos}.

It should also be noted that an optimal quantizer can be defined just by its codebook $\mathcal{C}$, since the encoder can be optained from the codebook by applying the nearest neighbour condition.

Finally, it is possible to rewrite the average distortion $D_{avg}$ from Equation~\ref{eq:D_avg} for a vector quantizer as follows:
\begin{align}
D_{avg} &= \sum_{i=1}^{N} E[ \|X - x_i\| | X \in R_i] P(X \in R_i) \\
&= \sum_{i=1}^{N} \int_{R_i} \|x - x_i\| p(x) dx
\end{align}

\subsection{Channel Optimized Quantization}
\label{sec:channel_optimized}
This subsection will discuss how quantization can be using in communication over noisy channels. As with regular quantization, a point is first encoding on an index. This time, however, the index is transmitted over a channel to a receiver. Channel interference may influence the received index, so the index that was sent may not necessarily correspond with the index that was transmitted. Channel optimized quantization refers a quantizer that is designed to be optimized for a given channel and source distribution. The goal in channel optimized quantizer design is to minimize the distortion between the original source and reconstruction at the receiver.

The codebook $\mathcal{C}$ and encoder $\mathcal{E}$ are still defined in the same way as in regular quantization. Nearest neighbor and centroid conditions apply as well in the same way. With channel optimized quantization, however, it is necessary to consider the channel when deriving the conditions of optimality.

The channel that will be considered will be assumed to be memoryless with channel transition probabilities denoted by $P(j|i)$, that is, the probability of receiving index $j$ at the receiver given index $i$ was sent from the transmitter, where both $i$ and $j$ are in the set $\{1,\ldots,N\}$. If the codevector to index mapping is denoted by $b = (b(x_1),\ldots,b(x_N))$, then the probability of decoding code vector $x_j$ given codevector $x_i$ was transmitted is $P(b(x_j)|b(x_i))$.

The nearest neighbor and centroid conditions for the new system can now be given as follows:
\begin{theorem}
Amongst the set of all $N$-level quantizers with encoding function $\mathcal{E}$, and channel transition probabilities $\{P(j|i)\}$ the quantizer with code vectors
\begin{equation}
  x_j = \sum_{i=1}^N E[X | X \in R_i]P(b(x_j)|b(x_i))
\end{equation}
for all $j \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
This is similar to before with regular quantization, but note how the centroid depends on both the encoding regions and channel probabilities. Next, the nearest neighbor condition is:
\begin{theorem}
Consider all $N$-level quantizers with codebook $\mathcal{C}$, and channel transition probabilities $\{P(j|i)\}$. The quantizer with encoding regions satisfying
\begin{equation}
R_j \subset \{x : \sum_{i=1}^N \| x - x_i \|P(j|b(x_i)) \le \sum_{i=1}^N \| x - x_i \|P(k|b(x_i)) \text{ for all } k = 1,\ldots,N \}
\end{equation}
for all $j \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
The interpretation for both conditions of optimality are the same as with regular quantization, although the nearest neighbor and centroid conditions are now taken to be weighed across all encoding regions and code vectors respectively. The average distortion for an $N$-level channel optimized vector quantizer is given by
\begin{align}
D_{avg} &= \sum_{i=1}^{N} \sum_{j=1}^{N} E[ \|X - x_j\| | X \in R_i] P(X \in R_i) P(b(x_j)|b(x_i))\\
&= \sum_{i=1}^{N} \sum_{j=1}^{N} P(b(x_j)|b(x_i)) \int_{R_i} \|x - x_j\| p(x) dx
\end{align}
One advantage of using channel optimized quantization in practice is that the scheme performs source and channel coding in a single step, rather than using two separate systems. That is, it is able to compress the incoming data (source coding) and encode the trans missioned information so that it is resilient to channel interference (channel coding). The advantage to jointly performing source and channel coding jointly rather than in tandem is that it is typically more computationally efficient.

\subsection{Quantizer Design Algorithms}
\label{sec:quant_design_algos}
Quantizer design algorithm will now be discussed in this section. One advantage to using the proposed algorithms is that they do not require knowledge of the underlying source distribution, and instead can be applied to a training set. This is an important practical consideration, which allows these algorithms to be easily applied to images.

When discusses the nearest neighbor and centroid conditions, it was remarked that the nearest neighbor condition can be though of as optimizing the encoder for a fixed decoder (the codebook), and the centroid condition can be though of as optimizing the decoder for a fixed encoder. By applying these two conditions iteratively one after the other, it should be expected that the algorithm will eventually converge onto a good quantizer. This is known as the Lloyd-Max quantizer design algorithm, and can be implemented as follows using a training set $\mathcal{T}$:

{\sc \noindent Lloyd-Max Algorithm}
\begin{enumerate}
\item Start with the codebook in some initial state
\item For each training point in $\mathcal{T}$, apply the nearest neighbor condition to find the code vector of lowest distortion in the codebook
\item For each code vector in $\mathcal{C}$, compute the centroid of the corresponding encoding region by using the set of points that mapped onto that code vector in the previous step
\item Return to step (2). Finish once the calculated distortion across the training set has converged
\end{enumerate}
One limitation of the Lloyd-Max algorithm is that is does not guarantee convergence onto a globally optimal quantizer. The quality of the quantizer after convergence is highly dependent on the initial state of the codebook. Improper initialization of the codebook may lead to suboptimal quantizers.

Another algorithm known as the Linde-Buzo-Gray (LBG) algorithm can be used in conjunction with the Lloyd-Max algorithm to improve initialization conditions in the hope of converging onto a better quantizer. In the (LBG) algorithm, code vectors are gradually introduced into the codebook rather than initializing them all at once. More code vectors are introduced only after the quantizer with the reduced codebook has converged. An overview of the LBG algorithm is as follows:

{\sc \noindent Linde-Buzo-Gray Algorithm}
\begin{enumerate}
\item Begin with an initial codebook $\mathcal{C}$ containing a single code vector placed at the centroid of the training set.
  \item Let $N$ denote the size of the codebook $\mathcal{C}$ and code vectors denoted by $x_i$. Construct a new codebook $\mathcal{C}^*$ of size $2N$ with code vectors $x_i^* = (1-\epsilon)x_i$, $x_{i+N} = (1+\epsilon)x_i$ for all $1 \le i \le N$ for some small $\epsilon$.
\item Apply the Lloyd-Max algorithm to the new codebook $\mathcal{C}^*$
\item Let $\mathcal{C}=\mathcal{C}^*$
\item Return to step (2). Finish once the desired codebook size has been reached
\end{enumerate}
There are a few remarks to make about the LBG algorithm. Firstly, purpose of shifting the code vectors by a factor of $\epsilon$ is just done for tie-breaking purposes when applying the nearest neighbor condition. Secondly, the final codebook size is restricted to a power of 2 since it is doubled during iteration. This is not much of an issue since it can actually allow for a convenient binary representation of the codebook indexes.

By using the LBG algorithm in conjunction with the Lloyd-Max algorithm, there is hopefully a better chance of constructing a good codebook. It should be noted however, that convergence onto an optimal solution is still not guaranteed.

Other methods of improving quantizer performance will now be discussed in the remaining two subsections.

\subsection{Binary Codeword Assignment}
\label{sec:code_assign}
An important difference between vector quantization and channel optimized vector quantization is the impact of choosing the binary codeword for each codevector. In vector quantization, this choice has no impact on the quantizer's performance. This is not true for channel optimized quantization however, since the choice of binary codeword directly influences the effect of the channel on the average distortion. In channel optimized quantization, the binary codewords for each codevector should be chosen so the average distortion is minimized.

Recalling the equation for average distortion for the noisy channel in Equation~\ref{eq:channel_dis}, it was shown in~\ref{awd} that the average distortion can be expressed as follows if it is assumed that the code book satisfies the centroid conditon:
\begin{equation}
\label{eq:codeword_distortion}
D_{avg}  = \sum_{i=1}^N \int_{R_i} \|x - x_i\|p(x)dx + \sum_{i=1}^N \sum_{j=1}^N P(x_i) P(b(x_j)|b(x_i)) \|x_i - x_j\|
\end{equation}
Notice how only the second term is dependent on the choice of binary codewords. Therefore, it is possible to minimize the average distortion in $b$ for a fixed code book by only considering this second term. This term may be easier to minimize in practice because it only depends on the transmission probabilities $P(x_i)$ and the distortions between code vectors, rather than directly depending on the source distribution.

One interpretation of Equation~\ref{eq:codeword_distortion} is that the first term corresponds to the amount of distortion caused by the initial quantization onto $x_i$, and the second term corresponds to the distortion caused by transmitting the index over the channel. The second term will be denoted by $D_C$ and is given by
\begin{equation}
  \label{eq:channel_dist}
D_C = \sum_{i=1}^N \sum_{j=1}^N P(x_i) P(b(x_j)|b(x_i)) \|x_i - x_j\|
\end{equation}
Minimizing \ref{eq:channel_dist} for a fixed codebook lies in the set of problems which are known as being NP-complete, which means they are computationally expensive to solve. Minimizing~\ref{eq:channel_dist} for a large codebook may not be practical in reality, but there are a number of methods to get a good approximate to the optimal solution. The method that will be used here is known as simulated annealing, and will be discussed in more detail in~\ref{sec:sim_anneal}.

\subsection{Bit Allocation and Transform Coding}
\label{sec:bit_alloc}
Additional quantization techniques will now be discusses with applications to images. Consider the problem where a block of scalars $X_1,\ldots,X_k$ (which are not necessairly i.i.d.) is to be scalar quantized. It is now desired to minimize the averall distortion for the block
\begin{equation}
D_{block} = E[\sum_{i=1}^k(X_i - Q(X_i))^2]
\end{equation}
under the constraint that the total number of bits is constrained to $B$ bits per block:
\begin{equation}
\sum_{i=1}^k \log_2 N_i \le B
\end{equation}
The previously discussed design algorithms can be applied to design each of the $k$ scalar quantizers, but the bits now need to be allocated amongst the quantizers such that the total distortion is minimized.

\section{Problem Description and Background}

A more rigorous description of the problem will be given. As previously mentioned, the system that will be analyzed will consist of two encoders that each transmit indecies over a channel to a single decoder. The decoder uses the received indecies to reconstruct the two original sources. It will be assumed that the two sources share some sort of dependance, so a joint decoder system can use both indecies to optain a better estimate of each of the two original source values. Moreover, both noisy and noiseless channels will be analyzed.

It will be assumed that the source random vector $(X,Y)\in\real^2$ is independent and identically distributed, such that $X$ and $Y$ share a joint density function. In general, the encoder $\mathcal{E}$ which maps $(X,Y)$ onto a pair of indecies $i$ and $j$ for transmission is given by

\begin{equation*}
    \mathcal{E} : \real^2\to\{1,\ldots,N_X\} \times \{1,\ldots,N_Y\}
\end{equation*}
where $N_X$ and $N_Y$ are the number of output levels for the two encoders. If the encoding of the two sources is done independently, $\mathcal{E}$ can be expressed using an encoder for each source:
\begin{gather*}
    \mathcal{E}_X : \real\to\{1,\ldots,N_X\} \\
    \mathcal{E}_Y : \real\to\{1,\ldots,N_Y\}
\end{gather*}
so that $\mathcal{E}(x,y) = (\mathcal{E}_X(x), \mathcal{E}_Y(y))$. If encoding is not done independently, it is said to be done jointly. If encoding is done jointly, then the encoding region associated with transmitting index $i$ and $j$ is denoted $R_{i,j}$. When encoding is done independently, the encoding regions are denoted $R_i$ for the $X$ encoder and $S_j$ for the $Y$ encoder. Notice that using joint encoder notation, $R_{i,j} = R_i \times S_j$.

At the decoder, the two received indecies $k$ and $l$ are used to estimate $X$ and $Y$. The codebooks $\mathcal{C}_X$ and $\mathcal{C}_Y$ for $X$ and $Y$ are therefore both of size $N_X N_Y$. The codevectors corresponding to the received indexes $k$ and $l$ denoted by $x_{k,l} \in \mathcal{C}_X$ and $y_{k,l} \in \mathcal{C}_Y$ for $X$ and $Y$ respectively. Decoding of $X$ and $Y$ is done independently if the estimate of $X$ only depends on $k$ and the estimate of $Y$ only depends on $l$. When decoding is not done independently, it is said to be done jointly. When decoding is done independently, let $x_k$ and $y_l$ denote the esimates of $X$ and $Y$ for received indecies $k$ and $l$. Notice when decoding is done independently
\begin{align}
x_k &= x_{k,j} \text{ for all } j=1,\ldots,N_Y \\
y_l &= y_{i,l} \text{ for all } i=1,\ldots,N_X
\end{align}
The distortion that will be used in this scheme will be the sum of the squared-error distortion accross both sources:
\begin{equation}
    D_{avg} = E[{(X-\hat{X})}^2 + {(Y-\hat{Y})}^2]
\end{equation}
where $\hat{X} \in \mathcal{C}_X$ and $\hat{Y} \in \mathcal{C}_Y$ represent the estimates of $X$ and $Y$ at the receiver.

In this paper, the following systems will be analysed:

\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
    \bf Sysgem & \bf Encoder & \bf Decoder & \bf Channel \\ \hline \hline
    \sysII & Independent & Independent & Noiseless \\ \hline
    \sysIIN & Independent & Independent & Noisy \\ \hline
    \sysIJ & Independent & Joint & Noiseless \\ \hline
    \sysIJN & Independent & Joint & Noisy \\ \hline
    \sysJJ & Joint & Joint & Noiseless \\ \hline
    \sysJJN & Joint & Joint & Noisy \\ \hline
    \end{tabular}
\end{center}

As was mentioned in the introduction, the focus of this project is on the analysis and implementation of the \sysIJ\ and \sysIJN. It is clear for $N_X$ and $N_Y$ fixed, any \sysII\ (\sysIIN) can be implemented using a \sysIJ\ (\sysIJN) system, and any \sysIJ\ (\sysIJN) system can be implemented using a \sysJJ\ (\sysJJN) system. Therefore, an optimal \sysII\ (\sysIIN) system can be  provide a lower bound on performance an optimal \sysIJ\ (\sysIJN) system for similar channel characteristics for the noisy systems. Likewise, an optimal \sysJJ\ (\sysJJN) system provides upper bounds on performance for an optimal \sysIJ\ (\sysIJN) systems with similar channel characteristics.

The purpose of this report is to investigate the design and implementation of the \sysIJ\ and \sysIJN\ systems and compare their performance against their respective upper and lower bounds mentioned above.

\subsection{Explicit Expressions for Average Distortion}
The average distortion for the six previously mentioned systems will now be presented using explicit expressions. Doing so will shed light on the problem, while also helping in the derivation of new conditions of optimality for the \sysIJ\ and \sysIJN systems.

These expressions are easily derived by conditioning on transmitted and received indexes. 

{\sc \noindent Independent Encoders And Decoders, No Noise (\sysII):}
\begin{equation}
    \label{eq:dist_indep_nonoise}
    D_{avg} = \sum_{i=1}^{N_X}E[{(X-x_i)}^2 | x \in R_i]P(x \in R_i) + \sum_{j=1}^{N_Y}E[{(Y-y_j)}^2 | y \in S_j]P(y \in S_j)
\end{equation}
Notice how the average distortion can be expressed as the sum of distortions between both sources. Moreover, each term can be minimized independently using two independent scalar quantizers: one for $X$ and one for $Y$.

{\sc \noindent Independent Encoders And Decoders, With Noise (\sysIIN):}
\begin{equation}
    \label{eq:dist_indep_noise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y}
    E[{(X-x_{k})}^2 + {(Y-y_{l})}^2 | X \in R_i, Y \in S_j]P(b(x_k),b(y_l)|b(x_i),b(y_j))P(X \in R_i, Y \in R_j)
\end{equation}
Unlike the previous system, the average distortion for the \sysIIN\ system can not be seperated into two seperate terms unless it is assumed that the two channels for the transmitted indexes are independent: $P(k,l|i,j) = P(k|i)P(l|j)$. Under this assumption we have
\begin{multline}
    \label{eq:dist_indep_chan}
    D_{avg} = 
    \sum_{i=1}^{N_X}\sum_{k=1}^{N_X} E[{(X-x_{k})}^2 | X \in R_i]P(b(x_k)|b(x_i))P(X \in R_i) + \\
    \sum_{j=1}^{N_Y}\sum_{l=1}^{N_Y} E[{(Y-y_{l})}^2 | Y \in S_j]P(b(y_l)|b(y_j))P(Y \in S_j) 
\end{multline}
Two independent channel optimized scalar quantizers can be independently designed to quantize $X$ and $Y$ in order to minimize~\ref{eq:dist_indep_chan}. Although the assumption that the $X$ and $Y$ channels are independent is not necessary in order to study the nature of the other channels, only independent channels will be studied in this paper so the systems can be more easily compared. The assumption that the channels are independent will not be assumed in the remaining analysis of the remaining systems however.

{\sc \noindent Joint Decoder And Joint Encoder, No Noise (\sysJJ):}
\begin{equation}
  \label{eq:dist_JJ}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | (X,Y) \in R_{i,j}]P((X,Y) \in R_{i,j})
\end{equation}
The above expression can be minimized using a 2-dimensional vector quantizer with codebook size $N_XN_Y$.

{\sc \noindent Joint Decoder And Joint Encoder, With Noise (\sysJJN):}
\begin{equation}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | (X,Y) \in R_{i,j}]P(b(x_k),b(y_l)|b(x_i),b(y_j))P((X,Y) \in R_{i,j})
\end{equation}
The above expression can be minimized uning a 2-dimensional channel optimized quantizer with codebook size $N_XN_Y$.

{\sc \noindent Independent Encoders And Joint Decoders, No Noise (\sysIJ):}
\begin{equation}
    \label{eq:dist_joint_nonoise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | X \in R_i, Y \in S_j]P(X \in R_i, Y \in S_j)
\end{equation}
Unlike with the \sysII\ system, it is not possible to seperate the above expression into two terms than can be independently minimized using to seperate quantizers since the estimates of $X$ and $Y$ depend on both indecies. Also notice the slight difference between this expression and that of~\ref{eq:dist_JJ}. In both systems it is possible to define the codevectors using both indecies $i$ and $j$, but in the \sysIJ\ system, the encoding regions ($R_i \times R_j$) are restricted in their shape. New conditions for optimiality will be derived for this system in the next subsection and the implementation of such a quantizer will be discussed in detail in the following sectoion.

{\sc \noindent Independent Encoders And Joint Decoders, With Noise (\sysIJN):}
\begin{equation}
    \label{eq:dist_joint_noise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | E_X(X) = i, E_Y(Y) = j]P(k,l|i,j)P(E_X(X) = i, E_Y(Y) = j)
\end{equation}

\end{document}

