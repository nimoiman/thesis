\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\linespread{1.2}
\usepackage{parskip}
\setlength{\parindent}{15pt}

\newtheorem{theorem}{Theorem}[section]

\newcommand{\real}{\mathbb{R}}
\newcommand{\sysIIN}{\mbox{$I \overset{N}{\rightarrow} I$}}
\newcommand{\sysII}{\mbox{$I \rightarrow I$}}
\newcommand{\sysIJN}{\mbox{$I \overset{N}{\rightarrow} J$}}
\newcommand{\sysIJ}{\mbox{$I \rightarrow J$}}
\newcommand{\sysJJN}{\mbox{$J \overset{N}{\rightarrow} J$}}
\newcommand{\sysJJ}{\mbox{$J \rightarrow J$}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\section{Introduction}

\section{Background}
This section covers the background material related to the project. In particular, scalar and vector quantization with conditions of optimality will be introduced in Section~\ref{sec:quantization}. In Section~\ref{sec:channel_optimized}, quantizers used for transmission of information over noisy channels will be discussed. In Section~\ref{sec:quant_design_algos}, it will be shown how the conditions of optimality can be use in quantizer design. In Section~\ref{sec:code_assign}, proper codeword assignment for transmission over noisy channels will be discussed. Finally, bit allocation and transform coding techniques will be covered in Section~\ref{sec:bit_alloc} with a focus on images. After the above framework is introduced, a formal problem description results will be covered in Section~\ref{sec:design}.

\subsection{Quantization}
\label{sec:quantization}
Quantization is the process of mapping data from some large set onto a smaller, finite set. Quantization is a form of \emph{lossy compression}, which means the data can not be exactly recovered after quantization is performed on it.

A quantizer with $N$ distinct output levels is called an $N$-level quantizer. The output levels of an $N$-level quantizer can be indexed from $1$ to $N$. The \emph{encoder} is a mapping which maps a point from the input space $\mathcal{S}$ onto one of these indices. The encoder mapping is denoted by $\mathcal{E}$ and can be expressed as
\begin{equation}
\mathcal{E} : \mathcal{S} \rightarrow \{1,\ldots,N\}
\end{equation}
The \emph{encoding region} $R_i$ is the set of points in $\mathcal{S}$ that the quantizer maps onto index $i$:
\begin{equation}
R_i = \{x \in \mathcal{S} : \mathcal{E}(x) = i\}
\end{equation}
The encoding regions partition $\mathcal{S}$ and uniquely define $\mathcal{E}$.

The encoder indices have associated points in $\mathcal{S}$ that are used to estimate the original value of the data source and are called \emph{code vectors}. The set of code vectors is called the \emph{codebook} and is denoted by $\mathcal{C}$. There is a code vector for each index $i \in \{1,\ldots,N\}$ and is denotes $x_i$.

Any $N$-level quantizer can be defined by the pair $(\mathcal{E}, \mathcal{C})$. Moreover, the quantizer mapping $Q : \mathcal{S} \rightarrow \mathcal{S}$ is given by
\begin{equation}
Q(x) = x_{\mathcal{E}(x)}
\end{equation}

A distortion measure $d(x,y) : \mathcal{S}^2 \rightarrow \mathbb{R}^+$ is used to measure the performance of a quantizer. The average distortion $D_{avg}$ for a random i.i.d. source $X$, distortion measure $d(x,y)$, and quantizer $Q$, is given by
\begin{equation}
  \label{eq:D_avg}
D_{avg} = E[d(X,Q(X))]
\end{equation}
An $N$-level quantizer is \emph{optimal} if its average distortion is less than or equal to the average distortion of all other $N$-level quantizer.
 
In general, finding an optimal quantizer is very difficult. There may be quantizers that are locally optimal, but not globally optimal.

For the rest of the report, it will be assumed that the input space $\mathcal{S}=\mathbb{R}^k$. A quantizer with $k=1$ is called a \emph{scalar quantizer}, and a quantizer with $k \ge 1$ is called a \emph{vector quantizer}. \emph{Squared-error distortion}, denoted by ${\bf {\| x - y \|}}^2$, will be used for the rest of the report. Squared-error distortion is defined as follows:
\begin{equation}
{\bf {\| x - y \|}}^2 = \sum_{i=1}^n{(x_i - y_i)}^2
\end{equation}
where $x_i$ and $y_i$ are the $i$th elements of $\bf x$ and $\bf y$.

Two conditions of optimality for optimal quantizers will now be discussed. Their application to quantizer design will be presented in Section~\ref{sec:quant_design_algos}. It should be mentioned that the following conditions of optimality can be derived for general distortion measures and input spaces, although the results will only be given for squared-error distortion over $\mathbb{R}^k$ in this report.

The first condition of optimality is the \emph{centroid condition}. The centroid condition states that each code vector should be placed at the centroid of its corresponding encoding region.

\begin{theorem}
\label{theo:cent_vq}
Amongst the set of all $N$-level quantizers with encoding function $\mathcal{E}$, the quantizer with code vectors
\begin{align}
  \label{eq:cent_vq}
x_i &= E[X | X \in R_i] \\
&= \frac{ \int_{R_i}xf(x)dx }{ \int_{R_i}f(x)dx }
\end{align}
for all $i \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}

The second condition of optimality is the \emph{nearest neighbor condition}. The nearest neighbor condition states that the encoder mapping $\mathcal{E}$ should be chosen so each point in $\mathbb{R}^k$ maps onto the code vector of lowest distortion. \begin{theorem}
Consider all $N$-level quantizers with codebook $\mathcal{C}$. The quantizer with encoding regions satisfying
\begin{equation}
R_i \subset \{x : \| x - x_i \| \le \| x - x_j \| \text{ for all } j = 1,\ldots,N \}
\end{equation}
for all $i \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}

The centroid and nearest neighbor conditions provide necessary conditions of optimality for an optimal quantizer, but they are not sufficient. A quantizer that satisfies both conditions of optimality is called a Lloyd-Max quantizer.  An optimal quantizer can be defined by its codebook $\mathcal{C}$, since the nearest neighbor condition can be applied to the codebook to determine the encoding regions. The use of the conditions of optimality in quantizer design will be discussed in Section~\ref{sec:quant_design_algos}.

It is possible to rewrite the average distortion $D_{avg}$ from (\ref{eq:D_avg}) for a vector quantizer as follows:
\begin{align}
D_{avg} &= \sum_{i=1}^{N} E[ \| \mathbf{X} -  \mathbf{x_i}\| | \mathbf{X} \in R_i] P(\mathbf{X} \in R_i) \\
&= \sum_{i=1}^{N} \int_{R_i} \|\mathbf{x} - \mathbf{x_i}\| p(\mathbf{x}) d\mathbf{x}
\end{align}

\subsection{Channel Optimized Quantization}
\label{sec:channel_optimized}
The use of quantization in communication systems with noisy channels will now be discussed. In this scenario, the index from the encoder is transmitted over a noisy channel to the receiver, but channel interference can influence which index is received. A channel optimized quantizer is designed according to the channel and source distribution in order to minimize the distortion between the source value and estimate at the receiver. Nearest neighbor and centroid conditions still apply for a channel optimized quantizer, but they now depend on both the channel properties and source distribution.

The channel that will be considered in this paper is assumed to be memoryless, which means the properties of the channel do not change after each channel use. It will also be assumed that the channel is binary, so only one of two values can be transmitted at a time. To transmit an index over the channel, the index is first mapped onto a string of binary symbols using a mapping $b$. This binary representation is then transmitted over the channel. The receiver used the inverse mapping of $b$ to obtain the received index. The probability of receiving index $j$ given index $i$ was transmitted is $P(b(j)|b(i))$. These probabilities are called the \emph{channel transition probabilities}.

The nearest neighbor and centroid conditions for the channel optimized quantizer are now given, beginning with the centroid condition.

\begin{theorem}
\label{theo:cent_covq}
Amongst the set of all $N$-level channel quantizers with encoding function $\mathcal{E}$, the quantizer with code vectors
\begin{equation}
  \label{eq:cent_covq}
  x_j = \frac{\sum_{i=1}^N E[X | X \in R_i]P(b(j)|b(i))}{\sum_{i=1}^N P( X \in R_i)P(b(j)|b(i))} 
\end{equation}
for all $j \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
This is similar to the centroid condition in Theorem~\ref{theo:cent_vq}, but it now depends on both the source distribution and channel probabilities. The nearest neighbor condition for channel optimized quantization is now given.

\begin{theorem}
Consider all $N$-level quantizers with codebook $\mathcal{C}$. The quantizer with encoding regions satisfying
\begin{equation}
R_j \subset \left\{x : \sum_{i=1}^N {\| x - x_i \|}^2P(b(j)|b(i)) \le \sum_{i=1}^N {\| x - x_i \|}^2P(b(k)|b(i)) \text{ for all } k = 1,\ldots,N \right\}
\end{equation}
for all $j \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}

The average distortion for an $N$-level channel quantizer is

\begin{align}
D_{avg} &= \sum_{i=1}^{N} \sum_{j=1}^{N} E[ {\|X - x_j\|}^2 | X \in R_i] P(X \in R_i) P(b(x_j)|b(x_i))\\
&= \sum_{i=1}^{N} \sum_{j=1}^{N} P(b(x_j)|b(x_i)) \int_{R_i} {\|x - x_j\|}^2 p(x) dx
\end{align}

One advantage of using channel optimized quantization in practice is that it performs source and channel coding in a single step. This means it is able to compress the incoming data while encoding it so it is resilient to channel interference at the same time. Performing source and channel coding jointly, rather than in tandem, is beneficial because it can be more computationally efficient.

\subsection{Quantizer Design Algorithms}
\label{sec:quant_design_algos}
The Lloyd-Max and Linde-Buzo-Gray quantizer design algorithms will be discussed in this section. These two algorithms rely on the previously discuses conditions of optimality. A big advantage of these algorithms is that they do not require the underlying source distribution, and can instead be applied to a training set taken from the source. Recall that an optimal quantizer can be defined by its codebook, since the encoder can be obtained from the codebook by applying the nearest neighbor condition.

The Lloyd-Max algorithm will now be discussed. Recall that the nearest neighbor condition optimizes the encoder for a fixed codebook, and the centroid condition optimizes the codebook for a fixed encoder. The Lloyd-Max algorithm iteratively applies the centroid and nearest neighbor conditions on a training set until the codebook converges. The Lloyd-Max algorithm is implemented using a training set $\mathcal{T}$ taken from the source distribution. An overview of the algorithm follows:

\medskip

{\noindent \bf Lloyd-Max Algorithm}
\begin{enumerate}
\item Start with the codebook in some initial state.
\item Apply the nearest neighbor condition to each point in $\mathcal{T}$. Let $S(i) \subset \mathcal{T}$ denote the set of points that map onto index $i$.
\item Move each code vector $x_i$ to the centroid of $S(i)$ by applying the centroid condition.
\item Return to step (2). Finish once the training set distortion has converged.
\end{enumerate}

One limitation of the Lloyd-Max algorithm is that is does not guarantee convergence to an optimal quantizer. The performance of the quantizer is dependent on the initial state of the codebook before starting the algorithm. It is important that the codebook is initialized properly to ensure convergence onto a good codebook.

The Linde-Buzo-Gray can be used in conjunction with the Lloyd-Max algorithm to improve initialization conditions to obtain a better codebook. In the LBG algorithm, the codebook initially contains a single code vectors placed at the centroid of the entire training set. Code vectors are iteratively split in two in between iterations of the Lloyd-Max algorithm until the final codebook size is obtained. A more detailed description of the algorithm follows:

\medskip

{\sc \noindent Linde-Buzo-Gray Algorithm}
\begin{enumerate}
\item Begin with an initial codebook $\mathcal{C}$ containing a single code vector placed at the centroid of the training set $\mathcal{T}$.
\item Let $N$ denote the size of the codebook $\mathcal{C}$ with code vectors denoted by $x_i$. Construct a new codebook $\mathcal{C}^*$ of size $2N$ with code vectors $x_i^* = (1-\epsilon)x_i$, $x_{i+N}^* = (1+\epsilon)x_i$ for all $1 \le i \le N$ and some small $\epsilon$.
\item Let $\mathcal{C}=\mathcal{C}^*$
\item Apply the Lloyd-Max algorithm to $\mathcal{C}$.
\item Return to step (2). Finish once the desired codebook size has been reached
\end{enumerate}

The LBG algorithm can provide better initialization conditions than can therefore results in a better quantizer. Convergence onto an optimal solution is still not guaranteed using the LBG algorithm, however.

Other methods of improving quantizer performance are discussed in the remaining two subsections.

\subsection{Binary Codeword Assignment}
\label{sec:code_assign}
In channel optimized quantization it is important that the index mapping $b$ is chosen appropriately in order to minimize distortion. Recall the equation for average distortion for the noisy channel in (\ref{eq:channel_dis}), it was shown in (\ref{awd}) that if the codebook satisfies the centroid condition, the average distortion can be expressed as
\begin{equation}
\label{eq:codeword_distortion}
D_{avg}  = \sum_{i=1}^N \int_{R_i} {\|x - x_i\|}^2p(x)dx + \sum_{i=1}^N \sum_{j=1}^N P(x_i) P(b(x_j)|b(x_i)) {\|x_i - x_j\|}^2
\end{equation}

Only the second term on the right-hand side is dependent the mapping $b$. Therefore, it is possible to minimize the average distortion in $b$ for a fixed codebook by considering only this term. This term may be easier to minimize in practice because it only depends on the probability of transmitting each code vector $P(x_i)$ and the squared-error distortion between the code vectors.

One interpretation to (\ref{eq:codeword_distortion}) is that the first term on the right-hand side quantifies the distortion by quantization onto $x_i$, and the second term corresponds to the distortion from transmitting the index over the channel. The second term will be denoted by $D_C$ and is given by
\begin{equation}
  \label{eq:channel_dist}
D_C = \sum_{i=1}^N \sum_{j=1}^N P(x_i) P(b(x_j)|b(x_i)) \|x_i - x_j\|
\end{equation}
Minimizing (\ref{eq:channel_dist}) for a fixed codebook lies in the set of problems which are NP-complete, which means they are computationally expensive to solve. This minimization may not be practical for a large codebook, but there are a number of methods to get a good approximate to the optimal solution. Simulated annealing will be used in implementation in this report, and will be discussed in more detail in~\ref{sec:sim_anneal}.

\subsection{Bit Allocation and Transform Coding}
\label{sec:bit_alloc}
Additional quantization techniques will now be discussed with application to images. A block of scalars $X_1,\ldots,X_k$ is to be quantized. Instead of applying vector quantization to be block, $k$ scalar quantizers will be used instead. One reason for doing this is because scalar quantizers can be more efficient in practice. The overall distortion of the block $D_{block}$ is
\begin{equation}
D_{block} = E\left[\sum_{i=1}^k(X_i - Q(X_i))^2\right]
\end{equation}
It is assumed that the quantized block must be constrained to $B$ bits per block:
\begin{equation}
\sum_{i=1}^k \log_2 N_i \le B
\end{equation}
The $k$ scalar quantizers can be design using the previously mentioned algorithms, but the bits now need to be allocated amongst the quantizers such that the total distortion is minimized. This is known as bit allocation, and optimal bit allocation can drastically reduce the block distortion.

\section{Problem Description}
As mentioned in the Introduction, the system to be analyzed in this report consists of two transmitter that each transmit an index over a channel to a single receiver. The receiver uses the received indices to estimate the two sources. It is assumed that the two sources are dependent, so a joint decoder which uses both indices to estimate each source should obtain better source estimates. Noisy and noiseless channels will be analyzed in this report.

It is assumed that each of the two sources is scalar valued. Let $(X,Y)\in\real^2$ denote the random source which has a density function $f(x,y)$. The encoder $\mathcal{E}$ maps a pair $(x,y) \in \mathbb{R}^2$ onto a pair of indices $i$ and $j$:

\begin{equation*}
    \mathcal{E} : \real^2\to\{1,\ldots,N_X\} \times \{1,\ldots,N_Y\}
\end{equation*}
where $N_X$ and $N_Y$ are the number of output levels for the two encoders. If $X$ and $Y$ are encoded independently, $\mathcal{E}$ can be expressed as two seperate encoders
\begin{gather*}
    \mathcal{E}_X : \real\to\{1,\ldots,N_X\} \\
    \mathcal{E}_Y : \real\to\{1,\ldots,N_Y\}
\end{gather*}
so that $\mathcal{E}(x,y) = (\mathcal{E}_X(x), \mathcal{E}_Y(y))$. If encoding is not done independently, it is said to be done jointly. Let $R_{i,j} \subset \mathbb{R}^2$ denote the encoding region associated with index $i$ and $j$. When encoding is performed independently, the encoding regions can be expressed as $R_{i,j} = R_i^X \times R_j^Y$ where $R_i^X$ is the encoding region for $\mathcal{E}_X$ and index $i$, and $R_j^Y$ is the encoding region for $\mathcal{E}_Y$ for index $j$.

Let $k$ denote the index received at the receiver corresponding to index $i$ and let $l$ denotes the index at the receiver corresponding to index $j$. The indices $k$ and $l$ are used to estimate $X$ and $Y$ by the decoder. Let 
\begin{equation}
\mathcal{C} = \{ (x_{k,l},y_{k,l}) : k = 1,\ldots,N_X, l = 1,\ldots,N_Y\}
\end{equation}
denote the codebook. The code vector pair $(x_{k,l},y_{k,l})$ corresponds to the estimates of $X$ and $Y$ given indices $k$ and $l$ were received. Decoding of $X$ and $Y$ is done independently if the estimate of $X$ only depends on index $k$ and the estimate of $Y$ only depends on $l$. In this case, let
\begin{align}
\mathcal{C}_X &= \{x_k : k = 1,\ldots,N_X\} \\
\mathcal{C}_Y &= \{y_l : l = 1,\ldots,N_Y\}
\end{align}
denote the codebooks for $X$ and $Y$. Note that $\mathcal{C} = \mathcal{C}_X \times \mathcal{C}_Y$ when decoding independently.

The sum of the squared-error distortion of both sources will be used to measure performance:
\begin{equation}
\label{eq:sys_dist}
    D_{avg} = E[{(X-\hat{X})}^2 + {(Y-\hat{Y})}^2]
\end{equation}
where $(\hat X, \hat Y) \in \mathcal{C}$ represent the estimates of $X$ and $Y$ at the receiver.

In this report, different combinations of joint and independent encoders and codebook, over noisy and noiseless channel will be considered. The table below shows the notation that will be used for the different combinations:
\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
    \bf System & \bf Encoder & \bf Codebook & \bf Channel \\ \hline \hline
    \sysII & Independent & Independent & Noiseless \\ \hline
    \sysIIN & Independent & Independent & Noisy \\ \hline
    \sysIJ & Independent & Joint & Noiseless \\ \hline
    \sysIJN & Independent & Joint & Noisy \\ \hline
    \sysJJ & Joint & Joint & Noiseless \\ \hline
    \sysJJN & Joint & Joint & Noisy \\ \hline
    \end{tabular}
\end{center}

% Rewrite this
As mentioned in the Introduction, the focus of this project is on the analysis and implementation of the \sysIJ\ and \sysIJN. It is clear for $N_X$ and $N_Y$ fixed, any \sysII\ (\sysIIN) can be implemented using a \sysIJ\ (\sysIJN) system, and any \sysIJ\ (\sysIJN) system can be implemented using a \sysJJ\ (\sysJJN) system. Therefore, an optimal \sysII\ (\sysIIN) system can be  provide a lower bound on performance an optimal \sysIJ\ (\sysIJN) system for similar channel characteristics for the noisy systems. Likewise, an optimal \sysJJ\ (\sysJJN) system provides upper bounds on performance for an optimal \sysIJ\ (\sysIJN) systems with similar channel characteristics.

The goal of the present study is to design and implement \sysIJ\ and \sysIJN\ quantizers and compare their performance against the other quantizer for this two-source system.

\subsection{Explicit Expressions for Average Distortion}
The average distortion in (\ref{eq:sys_dist}) will now be reexpressed using explicit expressions for the six systems. All of these expressions can be easily derived by conditioning on transmitted and received indices. 

\medskip
{\sc \noindent Independent Encoders And Decoders, No Noise (\sysII):}
\begin{equation}
    \label{eq:dist_indep_nonoise}
    D_{avg} = \sum_{i=1}^{N_X}E[{(X-x_i)}^2 | x \in R_i]P(x \in R_i) + \sum_{j=1}^{N_Y}E[{(Y-y_j)}^2 | y \in S_j]P(y \in S_j)
\end{equation}
The the average distortion can be expressed as the sum of distortions between both sources. Moreover, each term on he right-hand side can be minimized independently using two independent scalar quantizers: one for $X$ and one for $Y$.

\medskip
{\sc \noindent Independent Encoders And Decoders, With Noise (\sysIIN):}
\begin{equation}
    \label{eq:dist_indep_noise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y}
    E[{(X-x_{k})}^2 + {(Y-y_{l})}^2 | X \in R_i, Y \in S_j]P(b(x_k),b(y_l)|b(x_i),b(y_j))P(X \in R_i, Y \in R_j)
\end{equation}
Unlike the previous system, the average distortion for the \sysIIN\ system can not be separated into two separate terms unless it is assumed that the two channels are independent, or $P(b(k),b(l)|b(i),b(j)) = P(b(k)|b(i))P(b(l)|b(j))$. Under this assumption we have
\begin{multline}
    \label{eq:dist_indep_chan}
    D_{avg} = 
    \sum_{i=1}^{N_X}\sum_{k=1}^{N_X} E[{(X-x_{k})}^2 | X \in R_i]P(b(k)|b(i))P(X \in R_i) + \\
    \sum_{j=1}^{N_Y}\sum_{l=1}^{N_Y} E[{(Y-y_{l})}^2 | Y \in S_j]P(b(l)|b(j))P(Y \in S_j) 
\end{multline}
Two independent channel optimized scalar quantizers can be independently designed to quantize $X$ and $Y$ in order to minimize~(\ref{eq:dist_indep_chan}). Although the assumption that the two channels are independent is not necessary in order to study the nature of the other channels, only independent channels will be studied in this paper so the systems can be more easily compared. The assumption of channel independence will not be made in the analysis of the remaining systems however.

\medskip
{\sc \noindent Joint Decoder And Joint Encoder, No Noise (\sysJJ):}
\begin{equation}
  \label{eq:dist_JJ}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | (X,Y) \in R_{i,j}]P((X,Y) \in R_{i,j})
\end{equation}
The above expression can be minimized using a 2-dimensional vector quantizer with codebook size $N_XN_Y$.

\medskip
{\sc \noindent Joint Decoder And Joint Encoder, With Noise (\sysJJN):}
\begin{equation}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | (X,Y) \in R_{i,j}]P(b(k),b(l)|b(i),b(j))P((X,Y) \in R_{i,j})
\end{equation}
The above expression can be minimized using a 2-dimensional channel optimized quantizer with codebook size $N_XN_Y$.

\medskip
{\sc \noindent Independent Encoders And Joint Decoders, No Noise (\sysIJ):}
\begin{equation}
    \label{eq:dist_IJ}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | X \in R_i, Y \in S_j]P(X \in R_i, Y \in S_j)
\end{equation}
Notice the slight difference between this expression and that of~(\ref{eq:dist_JJ}). New conditions for optimality will be derived for this system in the next subsection and the implementation of such a quantizer will be discussed in detail in the following section. 

\medskip
{\sc \noindent Independent Encoders And Joint Decoders, With Noise (\sysIJN):}
\begin{equation}
    \label{eq:dist_IJN}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | X \in R_i, Y \in S_j]P(k,l|i,j)P(X \in R_i, Y \in S_j)
\end{equation}
Many of the analogies made between the \sysIJ\ system and \sysJJ\ systems made above also apply between the \sysIJN\ and \sysIJ\ systems. In particular, the expression for average distortion for the \sysIJN\ system can not be separated into two independent terms, and the encoders and joint decoder have to be designed together.

The new conditions of optimality used in the \sysIJ\ and \sysIJN\ systems will now be introduced. A detailed description of the design algorithm for these two systems will follow in the following section.

\subsection{Conditions of Optimality for Independent Encoder, Joint Decoder Systems}
Conditions of optimality for the \sysIJ\ and \sysIJN\ systems will now be discussed beginning with the centroid condition. Beginning with the \sysIJ\ system, one can see from (\ref{eq:dist_IJ}) that the optimal code vectors are given by
\begin{align}
x_{i,j} &= E[X | X \in R_i, Y \in R_j] \\
&= \frac{ \int_{S_j}\int_{R_i}xf(x,y)dxdy }{ \int_{S_j}\int_{R_i}f(x,y)dxdy } \\
\end{align}
and
\begin{align}
y_{i,j} &= E[Y | Y \in R_i, Y \in R_j] \\
&= \frac{ \int_{S_j}\int_{R_i}yf(x,y)dxdy }{ \int_{S_j}\int_{R_i}f(x,y)dxdy } \\
\end{align}
These equations are analogous to the equations for the centroid condition derived for the quantizer in (\ref{eq:cent_vq}). Similar equations can be given for the centroids for the \sysIJN\ system:
\begin{align}
  x_{k,l} &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(b(k),b(l)|b(i),b(j))E[X | X \in R_i, Y \in R_j] \\
  &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(k,l|i,j)\int_{S_j} \int_{R_i} xf(x,y)dxdy
\end{align}

and
\begin{align}
  y_{k,l} &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(b(k),b(l)|b(i),b(j))E[Y | X \in R_i, Y \in R_j] \\
  &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(b(k),b(l)|b(i),b(j))\int_{S_j} \int_{R_i} yf(x,y)dxdy
\end{align}
Again, these equations are similar to the centroid condition for the channel optimized vector quantizer in (\ref{eq:cent_covq}).

Note that in both cases, the codebooks $\mathcal{C}_X$ and $\mathcal{C}_Y$ can be determined independently. In practice, this means that applying the centroid condition for these systems is no different than for the previously analyzed systems. 

The nearest neighbor conditions can now be introduced beginning with the \sysII\ system. Let $d_X(x,i)$ denote the expected distortion given $X=x$ and index $i$ was encoded at the $X$ encoder:

\begin{equation}
d_X(x,i)= \sum_{j=1}^{N_Y} ( {(x-x_{i,j})}^2 + E[{(Y-y_{i,j})}^2|X=x,Y\in S_j])P(Y \in S_j|X=x) \end{equation}
Let $d_Y(y,j)$ denote a similar expression for $Y$:
\begin{equation}
d_Y(y,j)= \sum_{i=1}^{N_X} ( {(y-y_{i,j})}^2 + E[{(X-x_{i,j})}^2|Y=y,X\in R_i])P(X \in R_i|Y=y) \end{equation}

The nearest neighbor condition for the \sysIJ\ system is given as follows:

\begin{theorem}
  \label{theo:NN_IJ}
Amongst all \sysIJ\ systems with codebooks $\mathcal{C}_X$, $\mathcal{C}_Y$and $Y$ encoder $\mathcal{E}_Y$, the system whose $X$ encoder $\mathcal{E}_X$ has encoding regions satisfy
\begin{equation}
R_i \subset \{ x : d_X(x,i) \le d_X(x,j) \text{ for all } j=1,\ldots,N_X \}
\end{equation}
for all $i=1,\ldots,N_X$ is optimal.
\end{theorem}

A similar theorem also applied for $\mathcal{E}_X$, using $d_Y(y,j)$ instead of $d_X(x,i)$.

For the \sysIJN\ system, let denote $d_X^N(x,i)$ the expected distortion at the receiver given $X=x$ and index $i$ was transmitted:

\begin{equation}
d_X(x,i)= \sum_{j=1}^{N_Y} \sum_{k=1}^{N_X} \sum_{l=1}^{N_Y} ( {(x-x_{k,l})}^2 + E[{(Y-y_{k,l})}^2|X=x,Y\in S_j])P(Y \in S_j|X=x) P(k,l|i,j)
\end{equation}
Similarly, let $d_Y(y,j)$ denote the expected distortion at the receiver given $Y=y$ and index $j$ was transmitted:
\begin{equation}
d_Y(y,j)= \sum_{i=1}^{N_X} \sum_{k=1}^{N_X} \sum_{l=1}^{N_Y} ( {(y-y_{k,l})}^2 + E[{(X-x_{k,l})}^2|Y=y,X\in R_i])P(X \in R_i|Y=y) P(k,l|i,j)
\end{equation}

Theorem~\ref{theo:NN_IJ} can be adapted to the \sysIJN\ system by using $d_X^N(x,i)$ and $d_Y^N(y,j)$ instead of $d_X(x,i)$ and $d_Y(y,j)$.

\end{document}