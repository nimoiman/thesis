% !TEX root = main.tex
\section{Introduction}

\section{Background}
This section will cover the necessary background material related to the project. In particular, scalar and vector quantization will be introduced in Section~\ref{sec:quantization}, along with associated conditions of optimally. In Section~\ref{sec:channel_optimized}, quantizers used for transmission of information over noisy channels will be discussed. In Section~\ref{sec:quant_design_algos}, it will be shown how the theory derived in the previous two sections can be used in optimal quantizer design. In Section~\ref{sec:code_assign}, proper codeword assignment for transmission over noisy channels will be discussed. Finally, bit allocation and transform coding techniques with a focus on images will be covered in Section~\ref{sec:bit_alloc}.

After the above framework is introduced, a more formal problem description and theoretical results will be covered in the following section.
\subsection{Quantization}
\label{sec:quantization}
As previously mentioned, quantization is the process of mapping data from some large set onto a smaller, finite set. Quantization is a form of \emph{lossy compression}, which means the data can not be exactly recovered after quantization.

As the number of possible outputs, or \emph{output levels}, of a quantizer is finite, each quantized point can be represented by an index over a finite range. A quantizer with $N$ distinct output levels is called an $N$-level quantizer. 

If the data originally lies in some space $\mathcal{S}$, the encoder $\mathcal{E}$ can be defined as a mapping
\begin{equation}
\mathcal{E} : \mathcal{S} \rightarrow \{1,\ldots,N\}
\end{equation}
The \emph{encoding region} associated with index $i$ is the set of points in $\mathcal{S}$ that the quantizer maps onto index $i$ and is denoted $R_i$. It follows that
\begin{equation}
R_i = \{x \in \mathcal{S} : \mathcal{E}(x) = i\}
\end{equation}
The set of encoding regions partition $\mathcal{S}$ and uniquely defines $\mathcal{E}$.

Each index output of the encoder has a unique point in $\mathcal{S}$ corresponding to the estimate of the original value. These points are called \emph{code vectors}. The set of code vectors is called the \emph{codebook} and is denoted by $\mathcal{C}$. The code vector corresponding to index $i$ is denoted by $x_i$ for all $i \in \{1,\ldots,N\}$.

Notice that any $N$-level quantizer can be defined by the encoder-codebook pair $(\mathcal{E}, \mathcal{C})$. Moreover, the quantizer mapping $Q : \mathcal{S} \rightarrow \mathcal{C}$ is given by
\begin{equation}
Q(x) = x_{\mathcal{E}(x)}
\end{equation}
Quantizer performance and optimal quantizers will now be discussed. A distortion measure $d(x,y) : \mathcal{S}^2 \rightarrow \mathbb{R}^+$ is used to measure the performance of a quantizer. The average distortion $D_{avg}$ for a random i.i.d. source $X$, distortion measure $d(x,y)$, and quantizer $Q$, is given by
\begin{equation}
  \label{eq:D_avg}
D_{avg} = E[d(X,Q(X))]
\end{equation}
An $N$-level quantizer is \emph{optimal} if its average distortion is less than or equal to the average distortion of all other $N$-level quantizer with respect to the given distortion measure and source distribution.

In general, solving for an optimal quantizer is very difficult. Moreover, the underlying source distribution may not be known when designing the quantizer, such as when quantizing images. Iterative algorithms in quantizer design are used which operate on a training set of the source, rather than the source distribution itself. Two useful conditions of optimality used in quantizer design will now be discussed, and their application to quantizer design will be presented in Section~\ref{sec:quant_design_algos}.

Before discussed the conditions of optimality, and for the rest of the paper, it will be assumed that $\mathcal{S}=\mathbb{R}^n$. A quantizer with $n=1$ is called a \emph{scalar quantizer}, and a quantizer with $n \ge 1$ is called a \emph{vector quantizer}. The rate $R$ of an $N$-level vector quantizer, in bits, is
\begin{equation}
R = \frac{1}{n}\log_2 N
\end{equation}
The distortion measure is assumed to be the \emph{squared-error distortion}, denoted ${\bf \| x - y \|}$ and defined as follows
\begin{align}
d(\bf x, \bf y) &= {\bf \| x - y \|} \\
&= \sum_{i=1}^n{(x_i - y_i)}^2
\end{align}
where $x_i$ and $y_i$ are the $i$th coordinates of $\bf x$ and $\bf y$. It should also be mentioned that the following conditions of optimality can be derived for general distortion measures and input spaces, although this will not be discussed in this report.

The first condition of optimality is the \emph{centroid condition}. The centroid condition states that each code vector should be placed at the centroid of its corresponding encoding region. The formal statement is as follows:
\begin{theorem}
\label{theo:cent_vq}
Amongst the set of all $N$-level quantizers with encoding function $\mathcal{E}$, the quantizer with code vectors
\begin{align}
  \label{eq:cent_vq}
x_i &= E[X | X \in R_i] \\
&= \frac{ \int_{R_i}xf(x)dx }{ \int_{R_i}f(x)dx }
\end{align}
for all $i \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
It is important to note that although a quantizer satisfying the above condition is optimal, it is not necessarily uniquely optimal.

The second condition of optimality is the \emph{nearest neighbor condition}. The nearest neighbor condition states that the encoder mapping $\mathcal{E}$ should be chosen such that each point should be mapped onto the code vector which results in the lowest distortion. More formally, the nearest neighbor condition can be stated as follows:
\begin{theorem}
Consider all $N$-level quantizers with codebook $\mathcal{C}$. The quantizer with encoding regions satisfying
\begin{equation}
R_i \subset \{x : \| x - x_i \| \le \| x - x_j \| \text{ for all } j = 1,\ldots,N \}
\end{equation}
for all $i \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
The subset symbol in the above theorem indicates that ties between nearest neighbors can be broken arbitrarly.

The centroid and nearest neighbor conditions together provide necessary conditions of optimality for an optimal quantizer, although they are not sufficient. A quantizer that satisfies both conditions of optimality is called a Lloyd-Max quantizer. It is also important to note that the centroid condition specifies an optimal codebook for a fixed encoder, and the nearest neighbor condition specifies and optimal encoder for a fixed codebook. The use of these conditions of optimality in quantizer design will be discussed in Section~\ref{sec:quant_design_algos}.

It should also be noted that an optimal quantizer can be defined just by its codebook $\mathcal{C}$, since the encoder can be obtained from the codebook by applying the nearest neighbor condition.

Finally, it is possible to rewrite the average distortion $D_{avg}$ from Equation~\ref{eq:D_avg} for a vector quantizer in the following form:
\begin{align}
D_{avg} &= \sum_{i=1}^{N} E[ \|X - x_i\| | X \in R_i] P(X \in R_i) \\
&= \sum_{i=1}^{N} \int_{R_i} \|x - x_i\| p(x) dx
\end{align}

\subsection{Channel Optimized Quantization}
\label{sec:channel_optimized}
How quantization can be using in communication over noisy channels will now be discussed. As with quantization without channel noise, a point is first encoded onto an index. This time, however, the index is transmitted over a channel to a receiver. Channel interference may influence the received index, and so the transmitted index sent may not necessarily correspond with the receivded index. Channel optimized quantization refers a quantizer that is designed to be optimized for a given noisy channel and source distribution. The goal in channel optimized quantizer design is to minimize the distortion between the original source and reconstruction at the receiver.

The codebook $\mathcal{C}$ and encoder $\mathcal{E}$ are still defined in the same way as before. Nearest neighbor and centroid conditions also apply. With channel optimized quantization, however, it is necessary to consider the channel when deriving the conditions of optimality.

The channel that will be considered will be assumed to be memoryless with channel transition probabilities denoted by $P(j|i)$, that is, the probability of receiving index $j$ at the receiver given index $i$ was sent from the transmitter, where both $i$ and $j$ are in the set $\{1,\ldots,N\}$. If the code vector to index mapping is denoted by $b = (b(x_1),\ldots,b(x_N))$, then the probability of decoding code vector $x_j$ given the index for code vector $x_i$ was transmitted is $P(b(x_j)|b(x_i))$.

The nearest neighbor and centroid conditions for the new system can now be given as follows:
\begin{theorem}
\label{theo:cent_covq}
Amongst the set of all $N$-level quantizers with encoding function $\mathcal{E}$, and channel transition probabilities $\{P(j|i)\}$, the quantizer with code vectors
\begin{equation}
  \label{eq:cent_covq}
  x_j = \sum_{i=1}^N E[X | X \in R_i]P(b(x_j)|b(x_i))
\end{equation}
for all $j \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
This is similar to the previous centroid condtion in Theorem~\ref{theo:cent_vq}, but note how the centroid depends on both the encoding regions and channel probabilities. Next, the nearest neighbor condition is:
\begin{theorem}
Consider all $N$-level quantizers with codebook $\mathcal{C}$, and channel transition probabilities $\{P(j|i)\}$. The quantizer with encoding regions satisfying
\begin{equation}
R_j \subset \{x : \sum_{i=1}^N \| x - x_i \|P(j|b(x_i)) \le \sum_{i=1}^N \| x - x_i \|P(k|b(x_i)) \text{ for all } k = 1,\ldots,N \}
\end{equation}
for all $j \in \{1,\ldots,N\}$ has minimum distortion.
\end{theorem}
The interpretation for both conditions of optimality are the same as before, although the nearest neighbor and centroid conditions are now taken to be weighed across all encoding regions and code vectors respectively. The average distortion for an $N$-level channel optimized vector quantizer is given by
\begin{align}
D_{avg} &= \sum_{i=1}^{N} \sum_{j=1}^{N} E[ \|X - x_j\| | X \in R_i] P(X \in R_i) P(b(x_j)|b(x_i))\\
&= \sum_{i=1}^{N} \sum_{j=1}^{N} P(b(x_j)|b(x_i)) \int_{R_i} \|x - x_j\| p(x) dx
\end{align}
One advantage of using channel optimized quantization in practice is that the scheme performs source and channel coding in a single step, rather than using two separate systems. Which means a channel optimized quantizer is able to compress the incoming data (source coding) and encode the trans missioned information so that it is resilient to channel interference (channel coding). The advantage to jointly performing source and channel coding jointly, rather than in tandem, is that it is typically more computationally efficient.

\subsection{Quantizer Design Algorithms}
\label{sec:quant_design_algos}
The Lloyd-Max and Linde-Buzo-Gray quantizer design algorithms will be discussed in this section. One advantage of using these algorithms is that they do not require knowledge of the underlying source distribution, and instead can be applied to a training set. This is an important practical consideration, which allows these algorithms to be easily applied to images.

When discussing the nearest neighbor and centroid conditions, it was remarked that the nearest neighbor condition can be though of as optimizing the encoder for a fixed decoder (the codebook), and the centroid condition can be though of as optimizing the decoder for a fixed encoder. By applying these two conditions iteratively, one after the other, it should be expected that the algorithm will eventually converge onto a good quantizer. This is known as the Lloyd-Max algorithm, and is implemented using a training set $\mathcal{T}$ as follows:

{\sc \noindent Lloyd-Max Algorithm}
\begin{enumerate}
\item Start with the codebook in some initial state.
\item For each training point in $\mathcal{T}$, apply the nearest neighbor condition to find the code vector of lowest distortion in the codebook.
\item For each code vector in $\mathcal{C}$, compute the centroid of the corresponding encoding region by using the set of points that mapped onto that code vector in the previous step.
\item Return to step (2). Finish once the calculated distortion across the training set has converged.
\end{enumerate}
One limitation of the Lloyd-Max algorithm is that is does not guarantee convergence onto a globally optimal quantizer. The quality of the quantizer after convergence is highly dependent on the initial state of the codebook. Improper initialization of the codebook may lead to suboptimal quantizers.

Another algorithm is the so-called Linde-Buzo-Gray (LBG) algorithm. The LBG algorithm can be used in conjunction with the Lloyd-Max algorithm to improve initialization conditions with the goal of converging onto a good codebook. In the (LBG) algorithm, code vectors are gradually introduced into the codebook rather than initializing them all at once. More code vectors are introduced only after the quantizer with the reduced codebook has converged. An overview of the LBG algorithm is as follows:

{\sc \noindent Linde-Buzo-Gray Algorithm}
\begin{enumerate}
\item Begin with an initial codebook $\mathcal{C}$ containing a single code vector placed at the centroid of the training set.
  \item Let $N$ denote the size of the codebook $\mathcal{C}$ and code vectors denoted by $x_i$. Construct a new codebook $\mathcal{C}^*$ of size $2N$ with code vectors $x_i^* = (1-\epsilon)x_i$, $x_{i+N} = (1+\epsilon)x_i$ for all $1 \le i \le N$ for some small $\epsilon$.
\item Apply the Lloyd-Max algorithm to the new codebook $\mathcal{C}^*$
\item Let $\mathcal{C}=\mathcal{C}^*$
\item Return to step (2). Finish once the desired codebook size has been reached
\end{enumerate}
There are a few remarks to make about the LBG algorithm. Firstly, the code vectors are shifted by a factor of $\epsilon$ to break ties when applying the nearest neighbor condition. Secondly, the final codebook size is restricted to a power of 2 since it is doubled each iteration. This is not much of an issue because it can actually allow for a convenient binary representation of the codebook indexes.

By using the LBG algorithm in conjunction with the Lloyd-Max algorithm, there is hopefully a better chance of constructing a good codebook. It should be noted however, that convergence onto an optimal solution is still not guaranteed.

Other ways of improving quantizer performance are discussed in the remaining two subsections.

\subsection{Binary Codeword Assignment}
\label{sec:code_assign}
An important difference between vector quantization and channel optimized vector quantization is the impact of choosing the binary codeword for each code vector. In vector quantization, this choice has no impact on the quantizer's performance. This is not true for channel optimized quantization however, because the choice of binary codeword directly influences the effect of the channel on the average distortion. In channel optimized quantization, the binary codewords for each code vector should be chosen so the average distortion is minimized.

Recalling the equation for average distortion for the noisy channel in Equation~\ref{eq:channel_dis}, it was shown in~\ref{awd} that  if it is assumed that the code book satisfies the centroid condition the average distortion can be expressed as follows:
\begin{equation}
\label{eq:codeword_distortion}
D_{avg}  = \sum_{i=1}^N \int_{R_i} \|x - x_i\|p(x)dx + \sum_{i=1}^N \sum_{j=1}^N P(x_i) P(b(x_j)|b(x_i)) \|x_i - x_j\|
\end{equation}
Note how only the second term is dependent on the choice of binary codewords. Therefore, it is possible to minimize the average distortion in $b$ for a fixed code book by only considering this second term. This term may be easier to minimize in practice because it only depends on the probabiliy of transmitting each code vector $P(x_i)$ and the distortions between code vectors.

One interpretation to (\ref{eq:codeword_distortion}) is that the first term on the right-hand side quantifies the amount of distortion caused by the initial quantization onto $x_i$, and the second term corresponds to the distortion caused by transmitting the index over the channel. The second term will be denoted by $D_C$ and is given by
\begin{equation}
  \label{eq:channel_dist}
D_C = \sum_{i=1}^N \sum_{j=1}^N P(x_i) P(b(x_j)|b(x_i)) \|x_i - x_j\|
\end{equation}
Minimizing (\ref{eq:channel_dist}) for a fixed codebook lies in the set of problems which are known as being NP-complete, which means they are computationally expensive to solve. This minimization may not be practical for a large codebook, but there are a number of methods to get a good approximate to the optimal solution. The method that will be used here is known as simulated annealing, and will be discussed in more detail in~\ref{sec:sim_anneal}.

\subsection{Bit Allocation and Transform Coding}
\label{sec:bit_alloc}
Additional quantization techniques will now be discussed with application to images. Consider the problem of scalar quantizing each element in a block of scalaers $X_1,\ldots,X_k$ (which are not necessarily i.i.d.). The overall distortion of the block $D_{block}$ is to be minimized where
\begin{equation}
D_{block} = E[\sum_{i=1}^k(X_i - Q(X_i))^2]
\end{equation}
under the constraint that the bits are constrained to $B$ bits per block:
\begin{equation}
\sum_{i=1}^k \log_2 N_i \le B
\end{equation}
The previously discussed design algorithms can be applied to design each of the $k$ scalar quantizers, but the bits now need to be allocated amongst the quantizers such that the total distortion is minimized.

\section{Problem Description}

A more rigorous description of the problem will be given. As previously mentioned, the system that will be analyzed will consist of two encoders that each transmit indexes over a channel to a single decoder. The decoder uses the received indexes to reconstruct the two original sources. It will be assumed that the two sources share some sort of dependance, so a joint decoder system can use both indecies to optain a better estimate of each of the two original source values. Moreover, both noisy and noiseless channels will be analyzed.

It will be assumed that the source random vector $(X,Y)\in\real^2$ is independent and identically distributed, such that $X$ and $Y$ share a joint density function. In general, the encoder $\mathcal{E}$ which maps $(X,Y)$ onto a pair of indexes $i$ and $j$ for transmission is given by

\begin{equation*}
    \mathcal{E} : \real^2\to\{1,\ldots,N_X\} \times \{1,\ldots,N_Y\}
\end{equation*}
where $N_X$ and $N_Y$ are the number of output levels for the two encoders. If the encoding of the two sources is done independently, $\mathcal{E}$ can be expressed using an encoder for each source:
\begin{gather*}
    \mathcal{E}_X : \real\to\{1,\ldots,N_X\} \\
    \mathcal{E}_Y : \real\to\{1,\ldots,N_Y\}
\end{gather*}
so that $\mathcal{E}(x,y) = (\mathcal{E}_X(x), \mathcal{E}_Y(y))$. If encoding is not done independently, it is said to be done jointly. If encoding is done jointly, then the encoding region associated with transmitting index $i$ and $j$ is denoted $R_{i,j}$. When encoding is done independently, the encoding regions are denoted $R_i$ for the $X$ encoder and $S_j$ for the $Y$ encoder. Notice that using joint encoder notation, $R_{i,j} = R_i \times S_j$.

At the decoder, the two received indexes $k$ and $l$ are used to estimate $X$ and $Y$. The codebooks $\mathcal{C}_X$ and $\mathcal{C}_Y$ for $X$ and $Y$ are therefore both of size $N_X N_Y$. The code vectors corresponding to the received indexes $k$ and $l$ denoted by $x_{k,l} \in \mathcal{C}_X$ and $y_{k,l} \in \mathcal{C}_Y$ for $X$ and $Y$ respectively. Decoding of $X$ and $Y$ is done independently if the estimate of $X$ only depends on $k$ and the estimate of $Y$ only depends on $l$. When decoding is not done independently, it is said to be done jointly. When decoding is done independently, let $x_k$ and $y_l$ denote the estimates of $X$ and $Y$ for received indexes $k$ and $l$. Notice when decoding is done independently
\begin{align}
x_k &= x_{k,j} \text{ for all } j=1,\ldots,N_Y \\
y_l &= y_{i,l} \text{ for all } i=1,\ldots,N_X
\end{align}
The distortion that will be used in this scheme will be the sum of the squared-error distortion across both sources:
\begin{equation}
    D_{avg} = E[{(X-\hat{X})}^2 + {(Y-\hat{Y})}^2]
\end{equation}
where $\hat{X} \in \mathcal{C}_X$ and $\hat{Y} \in \mathcal{C}_Y$ represent the estimates of $X$ and $Y$ at the receiver.

In this paper, the following systems will be analyzed:

\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
    \bf System & \bf Encoder & \bf Decoder & \bf Channel \\ \hline \hline
    \sysII & Independent & Independent & Noiseless \\ \hline
    \sysIIN & Independent & Independent & Noisy \\ \hline
    \sysIJ & Independent & Joint & Noiseless \\ \hline
    \sysIJN & Independent & Joint & Noisy \\ \hline
    \sysJJ & Joint & Joint & Noiseless \\ \hline
    \sysJJN & Joint & Joint & Noisy \\ \hline
    \end{tabular}
\end{center}

As was mentioned in the introduction, the focus of this project is on the analysis and implementation of the \sysIJ\ and \sysIJN. It is clear for $N_X$ and $N_Y$ fixed, any \sysII\ (\sysIIN) can be implemented using a \sysIJ\ (\sysIJN) system, and any \sysIJ\ (\sysIJN) system can be implemented using a \sysJJ\ (\sysJJN) system. Therefore, an optimal \sysII\ (\sysIIN) system can be  provide a lower bound on performance an optimal \sysIJ\ (\sysIJN) system for similar channel characteristics for the noisy systems. Likewise, an optimal \sysJJ\ (\sysJJN) system provides upper bounds on performance for an optimal \sysIJ\ (\sysIJN) systems with similar channel characteristics.

The purpose of this report is to investigate the design and implementation of the \sysIJ\ and \sysIJN\ systems and compare their performance against their respective upper and lower bounds mentioned above.

\subsection{Explicit Expressions for Average Distortion}
The average distortion for the six previously mentioned systems will now be presented using explicit expressions. Doing so will shed light on the problem, while also helping in the derivation of new conditions of optimality for the \sysIJ\ and \sysIJN systems.

These expressions are easily derived by conditioning on transmitted and received indexes. 

{\sc \noindent Independent Encoders And Decoders, No Noise (\sysII):}
\begin{equation}
    \label{eq:dist_indep_nonoise}
    D_{avg} = \sum_{i=1}^{N_X}E[{(X-x_i)}^2 | x \in R_i]P(x \in R_i) + \sum_{j=1}^{N_Y}E[{(Y-y_j)}^2 | y \in S_j]P(y \in S_j)
\end{equation}
Notice how the average distortion can be expressed as the sum of distortions between both sources. Moreover, each term can be minimized independently using two independent scalar quantizers: one for $X$ and one for $Y$.

{\sc \noindent Independent Encoders And Decoders, With Noise (\sysIIN):}
\begin{equation}
    \label{eq:dist_indep_noise}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y}
    E[{(X-x_{k})}^2 + {(Y-y_{l})}^2 | X \in R_i, Y \in S_j]P(b(x_k),b(y_l)|b(x_i),b(y_j))P(X \in R_i, Y \in R_j)
\end{equation}
Unlike the previous system, the average distortion for the \sysIIN\ system can not be separated into two separate terms unless it is assumed that the two channels for the transmitted indexes are independent: $P(k,l|i,j) = P(k|i)P(l|j)$. Under this assumption we have
\begin{multline}
    \label{eq:dist_indep_chan}
    D_{avg} = 
    \sum_{i=1}^{N_X}\sum_{k=1}^{N_X} E[{(X-x_{k})}^2 | X \in R_i]P(b(x_k)|b(x_i))P(X \in R_i) + \\
    \sum_{j=1}^{N_Y}\sum_{l=1}^{N_Y} E[{(Y-y_{l})}^2 | Y \in S_j]P(b(y_l)|b(y_j))P(Y \in S_j) 
\end{multline}
Two independent channel optimized scalar quantizers can be independently designed to quantize $X$ and $Y$ in order to minimize~\ref{eq:dist_indep_chan}. Although the assumption that the $X$ and $Y$ channels are independent is not necessary in order to study the nature of the other channels, only independent channels will be studied in this paper so the systems can be more easily compared. The assumption that the channels are independent will not be assumed in the remaining analysis of the remaining systems however.

{\sc \noindent Joint Decoder And Joint Encoder, No Noise (\sysJJ):}
\begin{equation}
  \label{eq:dist_JJ}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | (X,Y) \in R_{i,j}]P((X,Y) \in R_{i,j})
\end{equation}
The above expression can be minimized using a 2-dimensional vector quantizer with codebook size $N_XN_Y$.

{\sc \noindent Joint Decoder And Joint Encoder, With Noise (\sysJJN):}
\begin{equation}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | (X,Y) \in R_{i,j}]P(k,l|i,j)P((X,Y) \in R_{i,j})
\end{equation}
The above expression can be minimized using a 2-dimensional channel optimized quantizer with codebook size $N_XN_Y$. Take note that the transmission indexes are no longer associated with code vectors, like in~\ref{eq:dist_indep_chan}, since a given index because the vectors are defined by two indexes, not just one.


{\sc \noindent Independent Encoders And Joint Decoders, No Noise (\sysIJ):}
\begin{equation}
    \label{eq:dist_IJ}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y} E[{(X-x_{i,j})}^2 + {(Y-y_{i,j})}^2 | X \in R_i, Y \in S_j]P(X \in R_i, Y \in S_j)
\end{equation}
Unlike with the \sysII\ system, it is not possible to separate the above expression into two terms than can be independently minimized using two separate quantizers. Notice the slight difference between this expression and that of~\ref{eq:dist_JJ}. In both systems it is possible to define the code vectors using both indexes $i$ and $j$, but in the \sysIJ\ system, the encoding regions ($R_i \times R_j$) are restricted in their shape. New conditions for optimality will be derived for this system in the next subsection and the implementation of such a quantizer will be discussed in detail in the following section. 

{\sc \noindent Independent Encoders And Joint Decoders, With Noise (\sysIJN):}
\begin{equation}
    \label{eq:dist_IJN}
    D_{avg} = \sum_{i=1}^{N_X}\sum_{j=1}^{N_Y}\sum_{k=1}^{N_X}\sum_{l=1}^{N_Y} E[{(X-x_{k,l})}^2 +
    {(Y-y_{k,l})}^2 | X \in R_i, Y \in S_j]P(k,l|i,j)P(X \in R_i, Y \in S_j)
\end{equation}
Many of the analogies made between the \sysIJ\ system and \sysJJ\ systems made above also apply between the \sysIJN\ and \sysIJ\ systems. In particular, the expression for average distortion for the \sysIJN\ system can not be separated into two independent terms, and the encoders and joint decoder have to be designed together.

The new conditions of optimality used in the \sysIJ\ and \sysIJN\ systems will now be introduced. A detailed description of the design algorithm for these two systems will follow in the following section.

\subsection{Conditions of Optimality for Independent Encoder, Joint Decoder Systems}
Conditions of optimality for the \sysIJ\ and \sysIJN\ systems will now be discussed beginning with the centroid condition. One can see from equation~\ref{eq:dist_IJ} that the optimal code vectors are given by
\begin{align}
x_{i,j} &= E[X | X \in R_i, Y \in R_j] \\
&= \frac{ \int_{S_j}\int_{R_i}xf(x,y)dxdy }{ \int_{S_j}\int_{R_i}f(x,y)dxdy } \\
\end{align}
and
\begin{align}
y_{i,j} &= E[Y | Y \in R_i, Y \in R_j] \\
&= \frac{ \int_{S_j}\int_{R_i}yf(x,y)dxdy }{ \int_{S_j}\int_{R_i}f(x,y)dxdy } \\
\end{align}
These equations are analogous to the equations for the centroid condition derived for the quantizer in Equation~\ref{eq:cent_vq}. Similar equations can be given for the centroids for the \sysIJN\ system:
\begin{align}
  x_{k,l} &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(k,l|i,j)E[X | X \in R_i, Y \in R_j] \\
  &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(k,l|i,j)\int_{S_j} \int_{R_i} xf(x,y)dxdy
\end{align}

and
\begin{align}
  y_{k,l} &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(k,l|i,j)E[Y | X \in R_i, Y \in R_j] \\
  &= \sum_{i=1}^{N_X} \sum_{j=1}^{N_Y} P(k,l|i,j)\int_{S_j} \int_{R_i} yf(x,y)dxdy
\end{align}
Again, these equations are similar to the centroid condition for the channel optimized vector quantizer in~\ref{eq:cent_covq}.

Notice that in both \sysIJ\ and \sysIJN\ systems, the codebooks $\mathcal{C}_X$ and $\mathcal{C}_Y$ for the $X$ and $Y$ sources can be determined independently. In practice, this means that applying the centroid condition for these systems is no different than for the previously analyzed systems. 

The nearest neighbor conditions can now be introduced beginning with the \sysII\ system. Let $d_X(x,i)$ denote the expected distortion given $X=x$ and index $i$ was encoded at the $X$ encoder for $\mathcal{C}_X$, $\mathcal{C}_Y$, $\mathcal{E}_X$ fixed:

\begin{equation}
d_X(x,i)= \sum_{j=1}^{N_Y} ( {(x-x_{i,j})}^2 + E[{(Y-y_{i,j})}^2|X=x,Y\in S_j])P(Y \in S_j|X=x) \end{equation}
Similarly, let $d_Y(y,j)$ denote the expected distortion at the receiver given $Y=y$ and index $j$ was transmitted for $\mathcal{C}_X$, $\mathcal{C}_Y$, $\mathcal{E}_X$ fixed:
\begin{equation}
d_Y(y,j)= \sum_{i=1}^{N_X} ( {(y-y_{i,j})}^2 + E[{(X-x_{i,j})}^2|Y=y,X\in R_i])P(X \in R_i|Y=y) \end{equation}

The nearest neighbor condition for the \sysIJ\ system is given as follows:

\begin{theorem}
Amongst all \sysIJ\ systems with codebooks $\mathcal{C}_X$, $\mathcal{C}_Y$and $Y$ encoder $\mathcal{E}_Y$, the system whose $X$ encoder $\mathcal{E}_X$ has encoding regions satisfy
\begin{equation}
R_i \subset \{ x : d_X(x,i) \le d_X(x,j) \text{ for all } j=1,\ldots,N_X \}
\end{equation}
for all $i=1,\ldots,N_X$ is optimal. Likewise, amongst all \sysIJ\ systems with codebooks $\mathcal{C}_X$, $\mathcal{C}_Y$ and $X$ encoder $\mathcal{E}_X$, the system whose $Y$ encoder $\mathcal{E}_Y$ has encoding regions satisfy
\begin{equation}
S_j \subset \{ y : d_Y(y,j) \le d_Y(y,i) \text{ for all } i=1,\ldots,N_Y \}
\end{equation}
for all $j=1,\ldots,N_Y$ is optimal.
\end{theorem}

A similar result also holds for the \sysIJN\ system. Let denote $d_X^N(x,i)$ the expected distortion at the receiver given $X=x$ and index $i$ was transmitted for $\mathcal{C}_X$, $\mathcal{C}_Y$, $\mathcal{E}_Y$ fixed:

\begin{equation}
d_X(x,i)= \sum_{j=1}^{N_Y} \sum_{k=1}^{N_X} \sum_{l=1}^{N_Y} ( {(x-x_{k,l})}^2 + E[{(Y-y_{k,l})}^2|X=x,Y\in S_j])P(Y \in S_j|X=x) P(k,l|i,j)
\end{equation}
Similarly, let $d_Y(y,j)$ denote the expected distortion at the receiver given $Y=y$ and index $j$ was transmitted for $\mathcal{C}_X$, $\mathcal{C}_Y$, $\mathcal{E}_X$ fixed:
\begin{equation}
d_Y(y,j)= \sum_{i=1}^{N_X} \sum_{k=1}^{N_X} \sum_{l=1}^{N_Y} ( {(y-y_{k,l})}^2 + E[{(X-x_{k,l})}^2|Y=y,X\in R_i])P(X \in R_i|Y=y) P(k,l|i,j)
\end{equation}

Using the above definitions, the nearest neighbor condition for the \sysII\ and \sysIIN\ systems can now be given:

\begin{theorem}

\end{theorem}
